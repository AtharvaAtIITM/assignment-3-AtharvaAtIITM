{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Data Exploration and Baseline Model [10 points]\n",
    "1. Load and Analyze the Dataset: Load the creditcard.csv dataset. This dataset has already been pre-processed using PCA, so no feature engineering is required. [1]\n",
    "2. Analyze Class Distribution: Print the class distribution (count of fraudulent vs. non-fraudulent transactions). A pie chart or bar plot would be a good visualization. Clearly state the degree of imbalance. [2]\n",
    "3. Baseline Model:\n",
    "    - Split the original dataset into training and testing sets. Crucially, ensure the test\n",
    "set retains its original imbalance. [2]\n",
    "    - Train a Logistic Regression classifier on the imbalanced training data. This will be your Model 1. [2]\n",
    "    - Evaluate the model's performance on the test set. Pay close attention to metrics robust to imbalance, such as Precision Recall, and the F1-score for the minority (fraudulent) class. Explain why accuracy is a misleading metric in this scenario. [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do you compose the clustering algorithm such that classification works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"datasets/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Resampling Approaches [25 points]\n",
    "1. Naive Oversampling (SMOTE): [5]\n",
    "    - Apply the SMOTE (Synthetic Minority Over-sampling Technique) algorithm to the training data. This method generates synthetic samples for the minority class. [3]\n",
    "    - Explain how SMOTE works and its potential limitations, such as generating noisy samples if the minority class is not well-defined. [2]\n",
    "    - Citation: N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, \"SMOTE: Synthetic Minority Over-sampling Technique,\" Journal of Artificial Intelligence Research, vol. 16, pp. 321â€“357, 2002.\n",
    "2. Clustering-Based Oversampling (CBO): [10]\n",
    "    - Explain the concept of using clustering for oversampling to ensure diversity and explain how it achieves this goal. [2]\n",
    "    - Use a clustering algorithm, such as K-Means, to identify a few clusters within the training data of the minority class only. You can choose k based on intuition or the Elbow Method. [5]\n",
    "    - Oversample from each minority cluster to create a new, balanced dataset. The goal is to ensure that all sub-groups are well-represented, thereby avoiding the creation of synthetic samples in regions with no actual data. [3]\n",
    "3. Clustering-Based Undersampling (CBU): [5]\n",
    "    - Explain the concept of using clustering for undersampling. The idea is to find sub-groups within the majority class and strategically remove instances to maintain a representative sample while reducing its size. [2]\n",
    "    - Use a clustering algorithm to find clusters within the training data of the majority class only. [5]\n",
    "    - Undersample from each cluster. For instance, you could remove samples from clusters that are closer to the minority class, or you could simply undersample each cluster proportionally to its size to preserve the original distribution of the majority class. [3]\n",
    "    - The final training set will comprise all instances of the minority class and the selected subset of majority class instances.\n",
    "\n",
    "Note: Libraries like imblearn provide convenient implementations for both SMOTE and clustering-based resampling methods, such as ClusterCentroids for undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Model Comparison and Analysis [15 points]\n",
    "1. Train and Evaluate Models: [5]\n",
    "    - Model 2 (SMOTE): Train a Logistic Regression classifier on the training data balanced with SMOTE. Evaluate its performance on the same, imbalanced test set from Part A.\n",
    "    - Model 3 (CBO): Train a Logistic Regression classifier on the training data balanced with your clustering-based oversampling approach. Evaluate its performance on the same, imbalanced test set.\n",
    "    - Model 4 (CBU): Train a Logistic Regression classifier on the training data balanced with your clustering-based undersampling approach. Evaluate its performance on the same, imbalanced test set.\n",
    "2. Performance Comparison [5]: Create a summary table or bar chart comparing the Precision, Recall, and F1-score of the four models (Baseline, SMOTE, CBO, and CBU) for the minority class.\n",
    "3. Conclusion and Recommendations: [5]\n",
    "    - Discuss the benefits and drawbacks of each resampling method for this problem. Which method performed the best and why?\n",
    "    - Explain how the clustering-based approaches address the limitations of a naive method like SMOTE.\n",
    "    - Conclude with a recommendation on which resampling strategy the company should adopt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da5401",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
